# 深度學習入門：基於Python的理論與實踐
## 第7章卷積神經網絡（CNN，Convolutional Neural Network）
### 7.1整體結構
全連接：相鄰層的所有神經元之間都有連接  
Affine（全連接層）後跟激活函數ReLu層或sigmoid層    
CNN：新增了卷積層（convolution層）和池化層（pooling層）    
連接順序：convolution-relu-（pooling，有時被省略）（Affine-ReLu被替代為convolution-relu）  
+ 靠近輸出的層中使用了之前的Affine-ReLU組合
+ 輸出層使用了之前的Affine-ReLU組合
### 7.2卷積層
#### 7.2.1全連接層存在的問題
**數據的形狀被忽視了** （3維形狀中可能隱藏有值得提取的本質模式，全連接層會忽視形狀，將全部的輸入數據作為相同的神經元（同一維度的神經元）處理，無法利用與形狀相關的信息）  
*卷積層可以保持不變*：當輸入數據是圖像時，卷積層會以3維數據的形式接收輸入數據，並同樣以3維數據的形式輸出至下一層  
+ 特征圖：卷積層的輸入輸出數據    
+ 輸入特征圖：卷積層的輸入數據
+ 輸出特征圖：卷積層的輸出數據
#### 7.2.2卷積計算  
卷積運算（乘積累加運算）：將各個位置上濾波器的元素和輸入的對應元素相乘，然後再求和，將結果保存到輸出的對應位置，將此過程在所有位置都進行一遍  
CNN中，**濾波器的參數對應全連接神經網絡中的權重**，且有偏置（向應用了濾波器的元素加上固定值）         
#### 7.2.3填充
填充（padding）：進行卷積層的處理之前，向輸入數據的周圍填入固定的數據（0）  
目的：調整輸出的大小  
#### 7.2.4步幅（應用濾波器的位置間隔）
增大步幅後，輸出大小會變小；增大填充後，輸出大小會變大  
輸出大小：  

$$OH = \frac{H+2P-FH}{S} +1$$  

$$OH = \frac{W+2P-FW}{S} +1$$

+ 輸入大小為（H,W）  
+ 濾波器大小為（FH,FW）
+ 輸出大小為（OH,OW）
+ 填充為P
+ 步幅為S  
（設定的值必須使公式分別可以除盡）
#### 7.2.5 3維數據的卷積計算
通道方向上有多個特征圖時，會按通道進行輸入數據和濾波器的卷積計算，將結果相加，從而得到輸出     
**在3維數據的卷積計算中，輸入數據和濾波器的通道數要設為相同的值**      
*濾波器大小可以設定為任意值，每個通道的濾波器大小要相同*
#### 7.2.6結合方塊思考  
輸出數據：1張特征圖（通道數為1的特征圖）  
多通道的輸出數據：多個濾波器（權重）FN→處理流             
濾波器的權重數據順序為（output_channel,input_channel,height,width）
#### 7.2.7批處理
將在各層間傳遞的數據保存為4維數據，按（batch_num,channel,height,width）的順序保存數據  
### 7.3池化層  
池化：縮小高、長方向上的空間的運算  
池化的窗口大小會和步幅設定成相同的值  
池化層的特征：  
+ 沒有要學習的參數：只是從目標區域中取最大值或平均值
+ 通道數不發生變化：輸入數據和輸出數據的通道數不會發生變化（計算是按通道獨立進行的）
+ 對微小的位置變化具有魯棒性（強壯）：輸入數據發生微小偏差時，池化仍會返回相同的結果
### 7.4卷積層和池化層的實現
#### 7.4.1 4維數組  
#### 7.4.2基於im2col的展開
NumPy中，訪問元素時最好不要用for語句（處理慢）  
1.im2col（函數）：將輸入數據展開以適合濾波器（對於輸入數據，將應用濾波器的區域橫向展開為1列）  
缺點：消耗更多內存（在濾波器的應用區域重疊的情況下，使用im2col展開後，展開後的元素個數會多於元方塊的元素個數）
2.將卷積層的濾波器縱向展開為1列，並計算2個矩陣的乘積
3.reshape輸出數據
#### 7.4.3卷積層的實現
#### 7.4.4池化層的實現
### 7.5 CNN的實現
### 7.6 CNN的可視化
### 7.7具有代表性的CNN









