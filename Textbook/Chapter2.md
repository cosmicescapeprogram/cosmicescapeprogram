# 深度學習入門：基於Python的理論與實踐  
## 第2章 感知機
### 2.1感知機模型（感知機接收多個輸入信號，輸出一個信號  ）
1）單層感知機    
階躍函數數學式： 

$$
y=
\begin{cases}
0,\omega_1 x_1 +\omega_2 x_2\leqslant0\\
1,\omega_1 x_1 +\omega_2 x_2>0\\ 
\end{cases}
$$ 

**權重起到控製信號流動的作用，偏置則直接調節輸出信號的大小**（*w用於控制各個信號的重要性，b用於控制神經元被激活的容易程度*）  
（感知機可以通過配置適當的權重和偏置，用以模擬不同功能的邏輯電路）

邏輯與： $$\omega_1=\omega_2=0.5$$ ,b=-0.7    
邏輯或： $$\omega_1=\omega_2=0.5$$ ,b=-0.2    
邏輯與非： $$\omega_1=\omega_2=-0.5$$ ,b=0.7    
2）多層感知機（通過疊加，組合多層的感知機，可以劃分非線性的輸出空間，得到更複雜、靈活的表示）  
異或XOR=AND(NAND,OR)    
### 2.2神經網絡模型
#### 2.2.1神經網絡的架構（通過學習，網絡能夠自動調整權重參數和偏置函數）
三層神經網絡：       
+ 輸入層
+ 隱藏層（可多層疊加）
+ 輸出層
#### 2.2.2激活函數（具有非線性能力的特殊函數）：將輸入函數的總和轉換為輸出信號     
（1） 階躍函數 Step function

$$
h=
\begin{cases}
0,\omega_1 x_1 +\omega_2 x_2\leqslant0\\
1,\omega_1 x_1 +\omega_2 x_2>0\\ 
\end{cases}
$$ 

（2）Sigmoid函數

$$h=\frac{1}{1+\exp{(-x)}}$$     

（3）ReLu函數

$$
h=
\begin{cases}
x,x>0\\     
0,x\leqslant0\\ 
\end{cases}
$$ 

（4）Softmax函數  

$$y_k =\frac{\exp{(a_k)}}{\sigma_{i=1}^n\exp{(a_i)}}$$










