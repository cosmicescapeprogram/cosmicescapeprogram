## 1.RNN（帶有記憶的時間順序的預測模型）     
循環神經網絡，是指在全連接神經網絡的基礎上增加了前後時序上的順序，可以更好地處理與時序相關的問題。  
公式：a<t> = tanh(Waa * a<t-1> + Wax * X<t> + b1) 記憶體現在今天的數據在前一天的基礎上得到(非線性激活函數）  
### 1.1 RNN的應用      
+ 語言模型：自然語言處理、機器翻譯、語音識別                       
+ 時間序列預測：股票價格、**氣象預測**、心電圖信號預測      
+ 生成模型：文本生成、音樂生產、藝術創作      
+ 強化學習：遊戲、機器人控製、決策製定      
### 1.2 RNN的缺陷      
主要缺陷：              
1. 長期依賴問題導致的**梯度消失**      
2. 梯度爆炸      
#### 1.2.1梯度消失和梯度爆炸的詳細公式推導   
有三种方法应对梯度消失问题：
        （1）合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。
        （2） 使用 ReLu 代替 sigmoid 和 tanh 作为激活函数。
        （3） 使用其他结构的RNNs，比如长短时记忆网络（LSTM）和 门控循环单元 （GRU），这是最流行的做法。
## 2. LSTM（長短時記憶網絡，常用于處理序列數據）        
與RNN（循環神經網絡）相比，LSTM引入了三個門（**輸入門、遺忘門、輸出門**和一個**細胞狀態**）  

*能夠更好地處理序列中的長期依賴關係* 
### 2.1 LSTM的模型結構 
pytorch訓練LSTM所使用參數：   
1. 調用：     
```
output,(h_n,c_n) = lstm (x, [ht_1, ct_1])，一般直接放入x就好，后面中括号的不用管
```
2. x給LSTM的參數：
```
x:[seq_length, batch_size, input_size],这里有点反人类，batch_size一般都是放在开始的位置

```
3. pytorch簡歷LSTM所需參數：
```
lstm = LSTM(input_size,hidden_size,num_layers)
```
### 2.2 LSTM相比RNN的優勢
劣勢：反向傳播的數學推導麻煩
優勢：解決梯度消失和梯度爆炸問題
